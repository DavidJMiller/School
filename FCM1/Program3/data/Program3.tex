\documentclass[12pt]{article}

%-------------PACKAGES------------- 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{pgfplots}
\usepackage{float}
\usepackage{braket}
\usepackage{titling}
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm,algpseudocode}
\usetikzlibrary{shapes,arrows,chains}
\usetikzlibrary[calc]

%-------------FORMATTING-------------
\setlength{\droptitle}{-7em} 
\setlength{\parindent}{0pt}
\def\LW{\dimexpr.25\linewidth-.5em} 
\tikzstyle{line} = [draw, -latex']
 
%--------------COMMANDS--------------
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
%\renewcommand{\qedsymbol}{\filledbox}

\DeclarePairedDelimiter \abs{\lvert}{\rvert}%
\DeclarePairedDelimiter \norm{\lVert}{\rVert}%

%------------ENVIRONMENTS------------- 
\newenvironment{theorem}[2][]{\begin{trivlist}
\item[{\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

%-------------CODE-STYLE------------
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{frame=tb,
	language=C++,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}

\lstset{
	morekeywords={end}
}

%------------------------------------ 
%---------START-OF-DOCUMENT----------
%------------------------------------
\begin{document}
 
\title{Program 3}
\author{David Miller \\ 
MAD5403: Foundations of Computational Math I} 
 
\maketitle

\section{Executive Summary}

In this program we investigate the accuracy, refinement process, and difference amongst quadrature methods. Different error tolerances are picked and the amount of sub-intervals needed to satisfy this tolerance are computed. Execution if the code proves these are conservative bounds, but bounds nonetheless. Refinement is applied with this error tolerance and stops whenever an appropriate mesh is created. All of this is shown in this report.

\section{Statement of Problem} 

Quadrature methods are pivotal in todays scientific community. They allow for efficient and accurate methods to obtain integral values when analytical methods can not be used. Among the myriad of quadrature methods we employ five: closed Newton-Cotes that uses two endpoints, closed Newton-Cotes that uses two endpoints and midpoint, open Newton-Cotes that uses the midpoint, open Newton-Cotes that uses points 1/2 and 2/3 in the interval, and gauss Legendre. We care to test these methods against different types of functions to verify that the correct execution of these methods are independent of the function but the error is dependent on the function. Using these functions we verify and analyze accuracy, error, and convergence of the quadrature methods. 

\section{Description of Mathematics}

Quadrature methods provide numerical approximations to definite integrals. This is done by first discretizing our domain $[a,b]$ into a uniform mesh $X = [x_0, \dots, x_n]$ where $a = x_0 < x_1 < \dots < x_n = b$. We then care about the error, so using the simple composite errors we can derive lower and upper bounds $E_i = f(a, b, n, f^{(n)}(\eta))$ for our composite methods where $n$ is the amount of sub-intervals and $\min f^{(n)}(\eta)$ and $\max f^{(n)}(\eta)$ are used to set a lower and upper bound, respectively. From $E_i$ we can also determine the amount of sub-intervals needed to satisfy an arbitrary error tolerance.

\newpage

\subsection{Closed Newton-Cotes - End Points (Trapezoidal)}

The simple composite closed Newton-Cotes quadrature that uses the two endpoints approximates the integral via
\begin{align}
I_1 = \displaystyle\int\limits_a^b f(x) \, dx \approx h\frac{f(b) - f(a)}{2}, \quad h = b - a
\end{align}
where the error is 
\begin{align}
E = -\frac{h^3}{12}f^{\prime\prime}(\eta), \quad \eta \in [a,b].
\end{align}
For the composite method we have that 
\begin{align}
I_n = \displaystyle\int\limits_a^b f(x) \, dx \approx \frac{h}{2}\bigg( f(a) + f(b) + 2\sum\limits_{i=1}^{n-1} f(x_i) \bigg), \quad h = \frac{b-a}{n}
\end{align}
from which we can derive the error via the single composite form 
\begin{align}
	E_i & = -\frac{h^3}{12}\sum\limits_{i=0}^{n-1} f^{\prime\prime}(\eta_i), \quad \eta_i \in [x_i, x_{i+1}] \\
	& = -\frac{h^3}{12}nf^{\prime\prime}(\mu), \quad \hskip 0.6cm \mu \in [a,b]
\end{align}
where we used the Mean Value Theorem for (5). We use this to bound our composite error 
\begin{align}
	\min\bigg(-\frac{h^3n}{12}\min f^{\prime\prime}(\mu),-\frac{h^3n}{12}\max f^{\prime\prime}(\mu) \bigg) \leq E_i \leq \max\bigg(-\frac{h^3n}{12}\min f^{\prime\prime}(\mu),-\frac{h^3n}{12}\max f^{\prime\prime}(\mu) \bigg).
\end{align}
From (6) we can derive an expression to tell us how many sub-intervals we need to satisfy some prescribed error tolerance. Simple algebraic manipulation of (4) and (5) yields
\begin{align}
	n = \sqrt{\frac{-(b-a)^3}{12E_i}f^{\prime\prime}(\mu)}
\end{align}
where $n$ will be between two values when we plug in $\min f^{\prime\prime}(\mu)$ and $\max f^{\prime\prime}(\mu)$ into (7).
Regarding refinement, closed Newton-Cotes using the two endpoints has $\alpha = 2$ and therefore doubles the amount of sub-intervals at each refinement. This can be verified graphically.
\begin{center}
\begin{tikzpicture}
	\draw (0,0) -- (1,0) -- (1,1) -- (0,1) -- (0,0);
	\draw (1,0) -- (2,0) -- (2,1) -- (1,1) -- (1,0);
	\draw (2,0) -- (3,0) -- (3,1) -- (2,1) -- (2,0);
	\node[text width=1cm] at (0.35,1.25) {$f_1$};
	\node[text width=1cm] at (1.35,1.25) {$f_2$};
	\node[text width=1cm] at (2.35,1.25) {$f_3$};
	\node[text width=1cm] at (3.35,1.25) {$f_4$};
	\node[text width=1cm] at (0.35,-0.25) {$x_1$};
	\node[text width=1cm] at (1.35,-0.25) {$x_2$};
	\node[text width=1cm] at (2.35,-0.25) {$x_3$};
	\node[text width=1cm] at (3.35,-0.25) {$x_4$};
	\path [line] (3.5, 0.5) -- node [text width=3.65cm,midway,above=0.5em ] {refinement ($\alpha = 2$)} (7.5, 0.5);
\end{tikzpicture} 
\begin{tikzpicture}
	\draw (0,0) -- (1,0) -- (1,1) -- (0,1) -- (0,0);
	\draw (1,0) -- (2,0) -- (2,1) -- (1,1) -- (1,0);
	\draw (2,0) -- (3,0) -- (3,1) -- (2,1) -- (2,0);
	\draw (3,0) -- (4,0) -- (4,1) -- (3,1) -- (3,0);
	\draw (4,0) -- (5,0) -- (5,1) -- (4,1) -- (4,0);
	\draw (5,0) -- (6,0) -- (6,1) -- (5,1) -- (5,0);
	\node[text width=1cm] at (0.35,1.25) {$f_1$};
	\node[text width=1cm] at (2.35,1.25) {$f_2$};
	\node[text width=1cm] at (4.35,1.25) {$f_3$};
	\node[text width=1cm] at (6.35,1.25) {$f_4$};
	\node[text width=1cm] at (0.35,-0.25) {$x^\prime_1$};
	\node[text width=1cm] at (1.35,-0.25) {$x^\prime_2$};
	\node[text width=1cm] at (2.35,-0.25) {$x^\prime_3$};
	\node[text width=1cm] at (3.35,-0.25) {$x^\prime_4$};
	\node[text width=1cm] at (4.35,-0.25) {$x^\prime_5$};
	\node[text width=1cm] at (5.35,-0.25) {$x^\prime_6$};
	\node[text width=1cm] at (6.35,-0.25) {$x^\prime_7$};
\end{tikzpicture}
\end{center}
where we can see reuse of evaluations can be used when refining.

\subsection{Closed Newton Cotes - 	End Points \& Midpoint (Simpson)}

The simple composite closed Newton-Cotes quadrature that uses the two endpoints and midpoint approximates the integral via
\begin{align}
I_1 = \displaystyle\int\limits_a^b f(x) \, dx \approx \frac{h}{3}(f(a) + 4f(\frac{b+a}{2}) + f(b)), \quad h = (b - a)/2
\end{align}
where the error is 
\begin{align}
E = -\frac{h^5}{90}f^{\prime\prime\prime\prime}(\eta), \quad \eta \in [a,b].
\end{align}
For the composite method we have that 
\begin{align}
I_n = \displaystyle\int\limits_a^b f(x) \, dx \approx \frac{h}{3}\bigg( f(a) + f(b) + \sum\limits_{i=1}^{n-1} 2f(x_i) + 4f(x_i + h) \bigg), \quad h = \frac{b-a}{2n},
\end{align}
from which we can derive the error via the single composite form 
\begin{align}
E_i & = -\frac{h^5}{90}\sum\limits_{i=0}^{n-1} f^{\prime\prime}(\eta_i), \quad \eta_i \in [x_i, x_{i+1}] \\
& = -\frac{h^5}{90}nf^{\prime\prime}(\mu), \quad \hskip 0.6cm \mu \in [a,b]
\end{align}
where we used the Mean Value Theorem for (12). We use this to bound our composite error 
\begin{align}
\min\bigg(-\frac{h^5n}{90}\min f^{\prime\prime\prime\prime}(\mu),-\frac{h^5n}{90}\max f^{\prime\prime\prime\prime}(\mu) \bigg) \leq E_i \leq \max\bigg(-\frac{h^5n}{90}\min f^{\prime\prime\prime\prime}(\mu),-\frac{h^5n}{90}\max f^{\prime\prime\prime\prime}(\mu) \bigg).
\end{align}
From (13) we can derive an expression to tell us how many sub-intervals we need to satisfy some prescribed error tolerance. Simple algebraic manipulation of (11) and (12) yields
\begin{align}
n = \sqrt[4]{\frac{-(b-a)^5}{2880E_i}f^{\prime\prime\prime\prime}(\mu)}
\end{align}
where $n$ will be between two values when we plug in $\min f^{\prime\prime}(\mu)$ and $\max f^{\prime\prime}(\mu)$ into (14).
Regarding refinement, closed Newton-Cotes using the two endpoints has $\alpha = 2$ and therefore doubles the amount of sub-intervals at each refinement. This can be verified graphically.
\begin{center}
	\begin{tikzpicture}
	\draw (0,0) -- (1,0) -- (1,1) -- (0,1) -- (0,0);
	\draw[dotted] (0.5,0) -- (0.5,1);
	\draw (1,0) -- (2,0) -- (2,1) -- (1,1) -- (1,0);
	\draw[dotted] (1.5,0) -- (1.5,1);
	\draw (2,0) -- (3,0) -- (3,1) -- (2,1) -- (2,0);
	\draw[dotted] (2.5,0) -- (2.5,1);
	\node[text width=1cm] at (0.35,1.25) {$f_1$};
	\node[text width=1cm] at (0.85,1.25) {$f_2$};
	\node[text width=1cm] at (1.35,1.25) {$f_3$};
	\node[text width=1cm] at (1.85,1.25) {$f_4$};
	\node[text width=1cm] at (2.35,1.25) {$f_5$};
	\node[text width=1cm] at (2.85,1.25) {$f_6$};
	\node[text width=1cm] at (3.35,1.25) {$f_7$};
	\node[text width=1cm] at (0.35,-0.25) {$x_1$};
	\node[text width=1cm] at (1.35,-0.25) {$x_2$};
	\node[text width=1cm] at (2.35,-0.25) {$x_3$};
	\node[text width=1cm] at (3.35,-0.25) {$x_4$};
	\path [line] (3.5, 0.5) -- node [text width=3.65cm,midway,above=0.5em ] {refinement ($\alpha = 2$)} (7.5, 0.5);
	\end{tikzpicture} 
	\begin{tikzpicture}
	\draw (0,0) -- (1,0) -- (1,1) -- (0,1) -- (0,0);
	\draw[dotted] (0.5,0) -- (0.5,1);
	\draw (1,0) -- (2,0) -- (2,1) -- (1,1) -- (1,0);
	\draw[dotted] (1.5,0) -- (1.5,1);
	\draw (2,0) -- (3,0) -- (3,1) -- (2,1) -- (2,0);
	\draw[dotted] (2.5,0) -- (2.5,1);
	\draw (3,0) -- (4,0) -- (4,1) -- (3,1) -- (3,0);
	\draw[dotted] (3.5,0) -- (3.5,1);
	\draw (4,0) -- (5,0) -- (5,1) -- (4,1) -- (4,0);
	\draw[dotted] (4.5,0) -- (4.5,1);
	\draw (5,0) -- (6,0) -- (6,1) -- (5,1) -- (5,0);
	\draw[dotted] (5.5,0) -- (5
	.5,1);
	\node[text width=1cm] at (0.35,1.25) {$f_1$};
	\node[text width=1cm] at (1.35,1.25) {$f_2$};
	\node[text width=1cm] at (2.35,1.25) {$f_3$};
	\node[text width=1cm] at (3.35,1.25) {$f_4$};
	\node[text width=1cm] at (4.35,1.25) {$f_5$};
	\node[text width=1cm] at (5.35,1.25) {$f_6$};
	\node[text width=1cm] at (6.35,1.25) {$f_7$};
	\node[text width=1cm] at (0.35,-0.25) {$x^\prime_1$};
	\node[text width=1cm] at (1.35,-0.25) {$x^\prime_2$};
	\node[text width=1cm] at (2.35,-0.25) {$x^\prime_3$};
	\node[text width=1cm] at (3.35,-0.25) {$x^\prime_4$};
	\node[text width=1cm] at (4.35,-0.25) {$x^\prime_5$};
	\node[text width=1cm] at (5.35,-0.25) {$x^\prime_6$};
	\node[text width=1cm] at (6.35,-0.25) {$x^\prime_7$};
	\end{tikzpicture}
\end{center}
where we can see reuse of evaluations can be used when refining. We just have to be careful with the weights of the evaluations when we reuse evaluations.

\subsection{Open Newton-Cotes - Midpoint (Midpoint)}

The simple composite open Newton-Cotes quadrature that uses the midpoint approximates the integral via
\begin{align}
I_1 = \displaystyle\int\limits_a^b f(x) \, dx \approx 2hf(\frac{a+b}{2}), \quad h = (b - a)/2
\end{align}
where the error is 
\begin{align}
E = \frac{h^3}{3}f^{\prime\prime}(\eta), \quad \eta \in [a,b].
\end{align}
For the composite method we have that 
\begin{align}
I_n = \displaystyle\int\limits_a^b f(x) \, dx \approx 2h\sum\limits_{i=0}^{n-1}f(\frac{x_i + x_{i+1}}{2}), \quad h = \frac{b-a}{2n}
\end{align}
from which we can derive the error via the single composite form 
\begin{align}
E_i & = \frac{h^3}{3}\sum\limits_{i=0}^{n-1} f^{\prime\prime}(\eta_i), \quad \eta_i \in [x_i, x_{i+1}] \\
& = -\frac{h^3}{3}nf^{\prime\prime}(\mu), \quad \hskip 0.6cm \mu \in [a,b]
\end{align}
where we used the Mean Value Theorem for (19). We use this to bound our composite error 
\begin{align}
\min\bigg(\frac{h^3n}{3}\min f^{\prime\prime}(\mu),\frac{h^3n}{3}\max f^{\prime\prime}(\mu) \bigg) \leq E_i \leq \max\bigg(\frac{h^3n}{3}\min f^{\prime\prime}(\mu),\frac{h^3n}{3}\max f^{\prime\prime}(\mu) \bigg).
\end{align}
From (20) we can derive an expression to tell us how many sub-intervals we need to satisfy some prescribed error tolerance. Simple algebraic manipulation of (18) and (19) yields
\begin{align}
n = \sqrt[]{\frac{(b-a)^3}{24E_i}f^{\prime\prime}(\mu)}
\end{align}
where $n$ will be between two values when we plug in $\min f^{\prime\prime}(\mu)$ and $\max f^{\prime\prime}(\mu)$ into (21).
Regarding refinement, closed Newton-Cotes using the two endpoints has $\alpha = 2$ and therefore doubles the amount of sub-intervals at each refinement. This can be verified graphically.
\begin{center}
	\begin{tikzpicture}
	\draw (0,0) -- (1,0) -- (1,1) -- (0,1) -- (0,0);
	\draw[dotted] (0.5,0) -- (0.5,1);
	\draw (1,0) -- (2,0) -- (2,1) -- (1,1) -- (1,0);
	\draw[dotted] (1.5,0) -- (1.5,1);
	\node[text width=1cm] at (0.85,1.25) {$f_1$};
	\node[text width=1cm] at (1.85,1.25) {$f_2$};
	\node[text width=1cm] at (0.35,-0.25) {$x_1$};
	\node[text width=1cm] at (1.35,-0.25) {$x_2$};
	\node[text width=1cm] at (2.35,-0.25) {$x_3$};
	\path [line] (2.5, 0.5) -- node [text width=3.65cm,midway,above=0.5em ] {refinement ($\alpha = 3$)} (6.5, 0.5);
	\end{tikzpicture} 
	\begin{tikzpicture}
	\draw (0,0) -- (1,0) -- (1,1) -- (0,1) -- (0,0);
	\draw[dotted] (0.5,0) -- (0.5,1);
	\draw (1,0) -- (2,0) -- (2,1) -- (1,1) -- (1,0);
	\draw[dotted] (1.5,0) -- (1.5,1);
	\draw (2,0) -- (3,0) -- (3,1) -- (2,1) -- (2,0);
	\draw[dotted] (2.5,0) -- (2.5,1);
	\draw (3,0) -- (4,0) -- (4,1) -- (3,1) -- (3,0);
	\draw[dotted] (3.5,0) -- (3.5,1);
	\draw (4,0) -- (5,0) -- (5,1) -- (4,1) -- (4,0);
	\draw[dotted] (4.5,0) -- (4.5,1);
	\draw (5,0) -- (6,0) -- (6,1) -- (5,1) -- (5,0);
	\draw[dotted] (5.5,0) -- (5.5,1);
	\node[text width=1cm] at (1.85,1.25) {$f_1$};
	\node[text width=1cm] at (4.85,1.25) {$f_2$};
	\node[text width=1cm] at (0.35,-0.25) {$x^\prime_1$};
	\node[text width=1cm] at (1.35,-0.25) {$x^\prime_2$};
	\node[text width=1cm] at (2.35,-0.25) {$x^\prime_3$};
	\node[text width=1cm] at (3.35,-0.25) {$x^\prime_4$};
	\node[text width=1cm] at (4.35,-0.25) {$x^\prime_5$};
	\node[text width=1cm] at (5.35,-0.25) {$x^\prime_6$};
	\node[text width=1cm] at (6.35,-0.25) {$x^\prime_7$};
	\end{tikzpicture}
\end{center}
where we can see reuse of evaluations can be used when refining. 

\subsection{Open Newton-Cotes - 1/3 \& 2/3 Points}

The simple composite open Newton-Cotes quadrature that uses points at 1/3 and 2/3 the interval approximates the integral via
\begin{align}
I_1 = \displaystyle\int\limits_a^b f(x) \, dx \approx \frac{3}{2}h(f(\frac{a+b}{3}) + f(\frac{2(a+b)}{3})), \quad h = (b - a)/3
\end{align}
where the error is 
\begin{align}
E = \frac{3}{4}h^3f^{\prime\prime}(\eta), \quad \eta \in [a,b].
\end{align}
For the composite method we have that 
\begin{align}
I_n = \displaystyle\int\limits_a^b f(x) \, dx \approx \frac{3}{2}h \sum\limits_{i=0}^{n-1} f(x_i+ h) + f(x_i + 2h/3), \quad h = \frac{b-a}{3n},
\end{align}
from which we can derive the error via the single composite form 
\begin{align}
E_i & = \frac{3}{4}\sum\limits_{i=0}^{n-1} f^{\prime\prime}(\eta_i), \quad \eta_i \in [x_i, x_{i+1}] \\
& = \frac{3}{4}h^3nf^{\prime\prime}(\mu), \quad \hskip 0.6cm \mu \in [a,b]
\end{align}
where we used the Mean Value Theorem for (26). We use this to bound our composite error 
\begin{align}
\min\bigg(\frac{3}{4}h^3n\min f^{\prime\prime}(\mu),\frac{3}{4}h^3n\max f^{\prime\prime}(\mu) \bigg) \leq E_i \leq \max\bigg(\frac{3}{4}h^3n\min f^{\prime\prime}(\mu),\frac{3}{4}h^3n\max f^{\prime\prime}(\mu) \bigg).
\end{align}
From (27) we can derive an expression to tell us how many sub-intervals we need to satisfy some prescribed error tolerance. Simple algebraic manipulation of (25) and (26) yields
\begin{align}
n = \sqrt[]{\frac{(b-a)^3}{36E_i}f^{\prime\prime}(\mu)}
\end{align}
where $n$ will be between two values when we plug in $\min f^{\prime\prime}(\mu)$ and $\max f^{\prime\prime}(\mu)$ into (28).
Regarding refinement, closed Newton-Cotes using the two endpoints has $\alpha = 2$ and therefore doubles the amount of sub-intervals at each refinement. This can be verified graphically.
\begin{center}
	\begin{tikzpicture}
	\draw (0,0) -- (1.5,0) -- (1.5,1) -- (0,1) -- (0,0);
	\draw[dotted] (0.5,0) -- (0.5,1);
	\draw[dotted] (1,0) -- (1,1);
	\draw (1.5,0) -- (3,0) -- (3,1) -- (1.5,1) -- (1.5,0);
	\draw[dotted] (2,0) -- (2,1);
	\draw[dotted] (2.5,0) -- (2.5,1);
	\node[text width=1cm] at (.75,1.25) {$f_1$};
	\node[text width=1cm] at (1.35,1.25) {$f_2$};
	\node[text width=1cm] at (2.25,1.25) {$f_3$};
	\node[text width=1cm] at (2.85,1.25) {$f_4$};
	\node[text width=1cm] at (0.35,-0.25) {$x_1$};
	\node[text width=1cm] at (1.85,-0.25) {$x_2$};
	\node[text width=1cm] at (3.35,-0.25) {$x_3$};
	\path [line] (3.5, 0.5) -- node [text width=3.65cm,midway,above=0.5em ] {refinement ($\alpha = 2$)} (7.5, 0.5);
	\end{tikzpicture} 
	\begin{tikzpicture}
	\draw (0,0) -- (1.5,0) -- (1.5,1) -- (0,1) -- (0,0);
	\draw[dotted] (0.5,0) -- (0.5,1);
	\draw[dotted] (1,0) -- (1,1);
	\draw (1.5,0) -- (3,0) -- (3,1) -- (1.5,1) -- (1.5,0);
	\draw[dotted] (2,0) -- (2,1);
	\draw[dotted] (2.5,0) -- (2.5,1);
	\draw (3,0) -- (4.5,0) -- (4.5,1) -- (3,1) -- (3,0);
	\draw[dotted] (3.5,0) -- (3.5,1);
	\draw[dotted] (4,0) -- (4,1);
	\draw (4.5,0) -- (6,0) -- (6,1) -- (4.5,1) -- (4.5,0);
	\draw[dotted] (5,0) -- (5,1);
	\draw[dotted] (5.5,0) -- (5.5,1);
	\node[text width=1cm] at (1.35,1.25) {$f_1$};
	\node[text width=1cm] at (2.35,1.25) {$f_2$};
	\node[text width=1cm] at (4.35,1.25) {$f_3$};
	\node[text width=1cm] at (5.35,1.25) {$f_4$};
	\node[text width=1cm] at (0.35,-0.25) {$x_1$};
	\node[text width=1cm] at (1.85,-0.25) {$x_2$};
	\node[text width=1cm] at (3.35,-0.25) {$x_3$};
	\node[text width=1cm] at (4.85,-0.25) {$x_4$};
	\node[text width=1cm] at (6.35,-0.25) {$x_5
		$};
	\end{tikzpicture}
\end{center}
where we can see reuse of evaluations can be used when refining.

\subsection{Two Point Gauss Legendre}

The Gauss-Legendre quadrature (in our case) uses two function evaluations and weights to approximate an integral and thus has degree of exactness 3. Given the general form $\int_{-1}^1 f(x) \, dx = \omega_1f(x_1) + \omega_2f(x_2)$, we have that
\begin{align}
\int\limits_{-1}^1 \, 1 dx = \omega_1 + \omega_2 & = 2 \\
\int\limits_{-1}^1 x \, dx = \omega_1x_1 + \omega_2x_2 & = 0 \\
\int\limits_{-1}^1 x^2 \, dx = \omega_1x_1^2 + \omega_2x_2^2 & = \frac{2}{3} \\
\int\limits_{-1}^1 x^3 \, dx = \omega_1x_1^3 + \omega_2x_2^3 & = 0
\end{align}
Solving this system yields
\begin{align}
	\omega_1 = \omega_2 = 1, \quad x_1 = \sqrt{\frac{1}{3}}, \quad x_2 = -\sqrt{\frac{1}{3}}
\end{align}
where we can map into our interval $[a,b]$ by 
\begin{align}
	x \mapsto \frac{a + b \pm \sqrt{\frac{1}{3}}(b-a)}{2}.
\end{align}
The errors for simple composite and composite are
\begin{align}
	E & = \frac{2^5(2!)^4}{5(3!)^3}f^{\prime\prime\prime\prime}(\eta) \\
	E_i & =  \frac{(\frac{b-a}{n})^5(2!)^4}{5(3!)^3}\sum\limits_{i=0}^{n-1}f(\eta_i), \quad \eta_i \in [x_i, x_{i+1}] \\
	& = \frac{(2!)^4}{5(3!)^3}(\frac{b-a}{n})^5nf^{\prime\prime\prime\prime}(\mu), \hskip .175cm \mu \in [a,b]
\end{align}
To get an estimate on the number of sub-intervals we algebraically manipulate (37) to get
\begin{align}
	n = \sqrt[4]{\frac{(b-a)^5}{4320E_i}f^{\prime\prime\prime\prime}}(\mu)
\end{align}
It's important to note that there is no $\alpha$ for refinement so mesh doubling is applied during refinement. The function values must just be computed at the new values.

\newpage

\section{Description of the Algorithm and Implementation}

\textbf{NOTE:} The current implementation of the code terminates refinement when a certain amount of refinements are done, not when a certain error tolerance is reached. However the code can easily be changed to check against error to terminate the refinement when we have a good enough mesh. \\

For the following algorithms we have 

\begin{enumerate}
	\item $f(x)$ with discretized domain $X = [x_0, \dots, x_n]$ such that $a = x_0 < x_n = b$.
	\item Error tolerance $\epsilon$ ($\epsilon$ = $-1$ if we do not want to refine)
	\item Analytic solution $I^* = \int\limits_a^b f(x) \, dx$
\end{enumerate}
where these are used to numerically compute the integral of $f(x)$. 

\subsection{Trapezoidal Algorithm}

\textbf{Inputs:} Mesh $X$, Function $f(x)$, Max error tolerance $\epsilon$, Analytic solution $I^*$ \\
\textbf{Outputs:} Quadrature result $I$ \\
\textbf{Storage:} Mesh $X$ = $[x_0, \dots, x_n]$ is $\mathcal{O}(n)$, Multiple temporary variables each $\mathcal{O}(1)$ \\
\textbf{Time:} For Loop at line 3 is $\mathcal{O}(n)$, While loop at line 6 is $\mathcal{O}(2^{r}nr)$, Everything else is $\mathcal{O}(1)$

\begin{algorithm}[H]
	\caption{Trapezoidal Method}
		\begin{algorithmic}[1]
			\State{$h \leftarrow x_1 - x_0, \, refinement \leftarrow 0, \, error \leftarrow \infty$}
			\State{$I \leftarrow \frac{1}{2}(f(x_0) + f(x_n))$}
			\For{$i = 1:1:n-1$}
			\State{$I \leftarrow I + f(x_i)$}
			\EndFor
			\While{error $> \epsilon$}
			\State{$h \leftarrow \frac{h}{2}$}
			\For{$i = 1:1:2^{refinement}(n - 1)$}
			\State{$I \leftarrow I + f(x_0 + (2i-1)h)$}
			\EndFor
			\State{refinement $\leftarrow$ refinement $+ 1$}
			\State{error $\leftarrow \abs{I^* - I*h}$}
			\EndWhile
			\State{$I \leftarrow I h$} \\
			\Return{$I$}
		\end{algorithmic}
\end{algorithm}

For the Trapezoidal algorithm the factor $2^rnr$ comes from the refinement. The $2^rn$ is the size of the new mesh at refinement iteration $r$. The code may seem to have a worst case of exponential but in fact it is just the size of a refined mesh, so it is still linear with respect to the size of the biggest mesh being evaluated.

\newpage

\subsection{Simpson Algorithm}

\textbf{Inputs:} Mesh $X$, Function $f(x)$, Max error tolerance $\epsilon$, Analytic solution $I^*$ \\
\textbf{Outputs:} Quadrature result $I$ \\
\textbf{Storage:} Mesh $X$ = $[x_0, \dots, x_n]$ is $\mathcal{O}(n)$, Multiple temporary variables each $\mathcal{O}(1)$ \\
\textbf{Time:} For loop at line 3 is $\mathcal{O}(n)$, While loop at line 6 is $\mathcal{O}(2^{r}nr)$, For loop at line 13 is $\mathcal{O}(2^rn)$, Everything else is $\mathcal{O}(1)$

\begin{algorithm}[H]
	\caption{Simpson Algorithm}
	\begin{algorithmic}[1]
		\State{$h \leftarrow x_1 - x_0, \, refinement \leftarrow 0, \, error \leftarrow \infty$}
		\State{$I \leftarrow f(x_0) + f(x_n) + f(x_1 + \frac{h}{2})$}
		\For{$i = 1:1:n-1$}
		\State{$I \leftarrow I + 2(f(x_i) _ f(x_i + \frac{h}{2}))$}
		\EndFor
		\While{error $> \epsilon$}
		\State{$h \leftarrow \frac{h}{2}$}
		\State{refinement $\leftarrow$ refinement $+ 1$}
		\For{$i = 1:2:2^{refinement}(n - 1)$}
		\State{$I^* \leftarrow I^* + 2f(x_0 + i\frac{h}{2})$}
		\EndFor
		\State{error $\leftarrow \abs{I^* - I*h/6}$}
		\EndWhile
		\For{$i=1:2:2^{refinement+1}(n-1)$}
		\State{$I \leftarrow I + 2f(x_0 + i\frac{h}{2})$}
		\EndFor
		\State{$I \leftarrow I\frac{h}{6}$} \\
		\Return{$I$}
	\end{algorithmic}
\end{algorithm}

For the Simpson algorithm the factor $2^rnr$ comes from the refinement. The $2^rn$ is the size of the new mesh at refinement iteration $r$. The code may seem to have a worst case of exponential but in fact it is just the size of a refined mesh, so it is still linear with respect to the size of the biggest mesh being evaluated.

\newpage

\subsection{Midpoint Algorithm}

\textbf{Inputs:} Mesh $X$, Function $f(x)$, Max error tolerance $\epsilon$, Analytic solution $I^*$ \\
\textbf{Outputs:} Quadrature result $I$ \\
\textbf{Storage:} Mesh $X$ = $[x_0, \dots, x_n]$ is $\mathcal{O}(n)$, Multiple temporary variables each $\mathcal{O}(1)$ \\
\textbf{Time:} For loop at line 2 is $\mathcal{O}(n)$, While loop at line 5 is $\mathcal{O}(3^rnr)$, Everything else is $\mathcal{O}(1)$

\begin{algorithm}[H]
	\caption{Midpoint Algorithm}
	\begin{algorithmic}[1]
		\State{$h \leftarrow x_1 - x_0, \, refinement \leftarrow 0, \, error \leftarrow \infty$}
		\For{$i = 0:1:n-1$}
		\State{$I \leftarrow I + f(x_i + \frac{h}{2})$}
		\EndFor
		\While{error $> \epsilon$}
		\State{$h \leftarrow \frac{h}{2}$}
		\State{refinement $\leftarrow$ refinement $+ 1$}
		\For{$i = 0:1:3^{refinement}(n - 1)$}
		\If{$i+2 \not\equiv 0 \text{ (mod 2)}$}
		\State{$I \leftarrow I + f(x_0 + h(i + \frac{1}{2}))$}
		\EndIf
		\EndFor
		\State{error $\leftarrow \abs{I^* - I*h}$}
		\EndWhile
		\State{$I \leftarrow Ih$} \\
		\Return{$I$}
	\end{algorithmic}
\end{algorithm}

For the Midpoint algorithm the factor $3^rnr$ comes from the refinement. The $3^rn$ is the size of the new mesh at refinement iteration $r$. The code may seem to have a worst case of exponential but in fact it is just the size of a refined mesh, so it is still linear with respect to the size of the biggest mesh being evaluated.

\newpage

\subsection{Open Newton-Cotes Algorithm (1/3 \& 2/3 Points)}

\textbf{Inputs:} Mesh $X$, Function $f(x)$, Max error tolerance $\epsilon$, Analytic solution $I$ \\
\textbf{Outputs:} Quadrature result $I$ \\
\textbf{Storage:} Mesh $X$ = $[x_0, \dots, x_n]$ is $\mathcal{O}(n)$, Multiple temporary variables each $\mathcal{O}(1)$ \\
\textbf{Time:} For loop at line 2 is $\mathcal{O}(n)$, While loop at line 5 is $\mathcal{O}(2^rnr)$, Everything else is $\mathcal{O}(1)$

\begin{algorithm}[H]
	\caption{Open Newton-Cotes Algorithm}
	\begin{algorithmic}[1]
		\State{$h \leftarrow x_1 - x_0, \, refinement \leftarrow 0, \, error \leftarrow \infty$}
		\For{$i = 0:1:n-1$}
		\State{$I \leftarrow I + f(x_i + \frac{h}{3}) + f(x_i + \frac{2h}{3})$}
		\EndFor
		\While{error $> \epsilon$}
		\State{$h \leftarrow \frac{h}{2}$}
		\State{refinement $\leftarrow$ refinement $+ 1$}
		\For{$i = 0:1:2^{refinement}(n - 1)$}
		\State{$I \leftarrow I + f(x_0 + h(2i + \frac{1}{3})) + f(x_0 + h(2i + \frac{5}{3}))$}
		\EndFor
		\State{error $\leftarrow \abs{I^* - I*h/2}$}
		\EndWhile
		\State{$I \leftarrow I\frac{h}{2}$} \\
		\Return{$I$}
	\end{algorithmic}
\end{algorithm}

For the Open Newton-Cotes (1/3 \& 2/3) algorithm the factor $2^rnr$ comes from the refinement. The $2^rn$ is the size of the new mesh at refinement iteration $r$. The code may seem to have a worst case of exponential but in fact it is just the size of a refined mesh, so it is still linear with respect to the size of the biggest mesh being evaluated.

\newpage

\subsection{Gauss-Legendre Algorithm}

\textbf{Inputs:} Mesh $X$, Function $f(x)$, Max error tolerance $\epsilon$, Analytic solution $I$ \\
\textbf{Outputs:} Quadrature result $I^*$ \\
\textbf{Storage:} Mesh $X$ = $[x_0, \dots, x_n]$ is $\mathcal{O}(n)$, Multiple temporary variables each $\mathcal{O}(1)$ \\
\textbf{Time:} For loop at line 2 is $\mathcal{O}(n)$, While loop at line 5 is $\mathcal{O}(2^rnr)$, Everything else is $\mathcal{O}(1)$

\begin{algorithm}[H]
	\caption{Gauss-Legendre Algorithm}
	\begin{algorithmic}[1]
		\State{$h \leftarrow x_1 - x_0, \, refinement \leftarrow 0, x_1 \leftarrow \sqrt{\frac{1}{3}}, \, x_2 \leftarrow -\sqrt{\frac{1}{3}}, \, error \leftarrow 0$}
		\For{$i=1:1:n-1$}
		\State{$x_1 \leftarrow (x_{i+1}+x_{i}+(x_{i+1}-x_i)\sqrt{1/3})/2$}
		\State{$x_2 \leftarrow (x_{i+1}+x_{i}-(x_{i+1}-x_i)\sqrt{1/3})/2$}
		\State{$I$ $\leftarrow$ $I + f(x_1)$ + $f(x_2)$}
		\EndFor
		\While{error $> \epsilon$}
		\State{$ I \leftarrow 0$}
		\State{$h \leftarrow h/2$}
		\For{$i=0:1:2^{refinement}(n-1)$}
		\State{$x_1 \leftarrow (2x_0 + ih + h\sqrt{1/3})/2$}
		\State{$x_2 \leftarrow (2x_0 + ih - h\sqrt{1/3})/2$}
		\State{$ I \leftarrow I + f(x_1) + f(x_2)$}
		\EndFor
		\EndWhile
		\State{$ I \leftarrow I*\frac{h}{2}$} \\
		\Return{$I$}
	\end{algorithmic}
\end{algorithm}

For the Gauss-Legendre algorithm the factor $2^rnr$ comes from the refinement. The $2^rn$ is the size of the new mesh at refinement iteration $r$. The code may seem to have a worst case of exponential but in fact it is just the size of a refined mesh, so it is still linear with respect to the size of the biggest mesh being evaluated.

\newpage

\section{Description of the Experiment Design and Results}

For this program we use the following functions
\begin{enumerate}
	\item $f_1(x) = e^x, \quad x \in [0,3]$
	\item $f_2(x) = e^{sin(2x)}cos(2x), \quad x \in [0. \frac{\pi}{3}]$
	\item $f_3(x) = tanh(x), \quad x \in [-2,1]$ 
	\item $f_4(x) = xcos(2\pi x), \quad x \in [0,3.5]$
	\item $f_5(x) = x + \frac{1}{x}, \quad x \in [0.1, 2.5]$
\end{enumerate}

Error analysis requires minimum and maximum values of the second and fourth derivatives of our functions so that we can give a bound. Below are the computed min and max using Wolfram$|$Alpha

\begin{table}[H]
	\centering
	\begin{tabular}{||l|c|c|c|c||}
		\cline{2-5}
		\multicolumn{1}{c|}{} & $\min f^{\prime\prime}(\mu)$ & $\max f^{\prime\prime}(\mu)$ & $\min f^{\prime\prime\prime\prime}(\mu)$ & $\max f^{\prime\prime\prime\prime}(\mu)$ \\
		\hline \hline
		$f_1(x) = e^x$ & 1 & 20.0855 & 1 & 20.0855 \\ \hline
		$f_2(x) = e^{(sin(2x))}cos(2x)$ & -16.2831 & 13.542 & -396.975 & 396.975 \\ \hline
		$f_3(x) = tanh(x)$ & -0.7698 & -0.0524725 & -4.0859 & 4.08589 \\ \hline
		$f_4(x) = xcos(2\pi x)$ & -81.1495 & 138.174 & -5454.91 & 3350.51 \\ \hline
		$f_5(x) = x + \frac{1}{x}$ & 0.128 & 2000 & 0.24576 & 2400000 \\ \hline
	\end{tabular}
	\caption{Min and max values of second and fourth derivatives for functions integrated}
\end{table}

For this program we want to do the following
\begin{itemize}
	\item Compute the interval size required for a relative error of 1\% and an absolute error of 0.001 and verify numerically via simple composite
	\item Verify that refinement stops when error is within the tolerance
	\item Show that some methods are inherently better than others
\end{itemize}
The next section presents the results and small discussions which prove the above points.

\newpage

\section{Results} 

\subsection{Function 1}

For $f(x) = e^x$ we have that
\begin{align} 
I = \int_0^3 e^x \, dx = 19.085537
\end{align}
where we use this to compare our quadrature results. If we want a relative error of 1\%, then our quadrature result $I_n$ must be in the interval $[.99I, 1.01I] \approx [18.894682, 19.276392]$ and if we want a relative error of 0.001 we want our quadrature $I_n$ to be in the interval $[19.084537,19.086537]$. Estimating amount of sub-intervals via equations (7), (14), (21), (28), and (38) for relative error of 1\% and absolute error of 0.001 we get

\begin{table}[H]
	\centering
	\begin{tabular}{||l|c|c||}
		\cline{2-3}
		\multicolumn{1}{c|}{} & Relative error 1\% & Absolute Error 0.001 \\ \hline \hline
		$n_{Trap}$ & 16 & 213 \\ \hline
		$n_{Simpson}$ & 2 & 7 \\ \hline
		$n_{Midpoint}$ & 11 & 151 \\ \hline
		$n_{Open2}$ & 9 & 123 \\ \hline
		$n_{Gauss-Legendre}$ & 2 & 6 \\ \hline
	\end{tabular}
	\caption{Number of sub-intervals for relative error of 1\% and absolute error of 0.001}
\end{table}
where we use $\max \abs{f^{\prime\prime}(\mu)}$ and $\max \abs{f^{\prime\prime\prime\prime}(\mu)}$ to get a conservative bound. We get the following results 
\begin{table}[H]
	\centering
	\begin{tabular}{||c|c|c|c|c|c|c|c|c|c||}
		\hline
		 $I_{16}^{Trap}$ & $I_{213}^{Trap}$ & $I_{2}^{Simpson}$ & $I_{7}^{Simpson}$ & $I_{11}^{Midpoint}$ & $I_{151}^{Midpoint}$ & $I_{9}^{Open2}$ & $I_{123}^{Open2}$ & $I_2^{GL}$ & $I_6^{GL}$ \\ \hline \hline
		.055863 & .000363 & .091463 & .000263 & .023037 & .000337 & .058737 & .000337 & .020837 & .000237 \\ \hline
	\end{tabular} 
	\caption{Errors of predicted $n$ for relative and absolute error}
\end{table}
It is easy to believe to see that these $n$ values satisfy our error requirements. It is easy to believe this since we used $\max f^{\prime\prime}(\mu)$ and $\max f^{\prime\prime\prime\prime}(\mu)$ to determine $n$ which yields a conservative number.

\begin{table}[H]
	\centering
	\begin{tabular}{||l|c|c|c|c|c||}
		\cline{2-6}
		\multicolumn{1}{c|}{} & Trapezoidal & Simpson & Midpoint & Open 2 & Gauss Legendre \\ \hline \hline
		$n=1/1$ & 31.6283 & 19.5061 & 13.4451 & 15.161 & 18.8101 \\ \hline
		$n=4/9$ & 19.9719 & 19.0876 & 18.9975 & 18.7913 & 19.0842 \\ \hline
		$n=16/81$ & 19.1414 & 19.0855 & 19.0844 & 19.0669 & 19.0855 \\ \hline
		$n=64/729$ & 19.089 & 19.0855 & 19.0855 & 19.0844 & 19.0855 \\ \hline
		$n=256/6561$ & 19.0858 & 19.0855 & 19.0855 & 19.0855 & 19.0855 \\ \hline
	\end{tabular}
	\caption{Results of quadrature with refinement. First $n$ is for methods with $\alpha = 1/2$ (and Gauss-Legendre) and second $n$ is for methods with $\alpha = 1/3$.}
\end{table}

\newpage

\subsection{Function 2}

For $f(x) = e^{(sin(2x))}cos(2x)$ we have that
\begin{align} 
I = \int_0^{\pi/3} e^{(sin(2x))cos(2x)} \, dx = 0.68872134
\end{align}
where we use this to compare our quadrature results. If we want a relative error of 1\%, then our quadrature result $I_n$ must be in the interval $[.99I, 1.01I] \approx [0.68183413, 0.69560855]$ and if we want a relative error of 0.001 we want our quadrature $I_n$ to be in the interval $[0.68772134,0.68972134]$. Estimating amount of sub-intervals via equations (7), (14), (21), (28), and (38) for relative error of 1\% and absolute error of 0.001 we get

\begin{table}[H]
	\centering
	\begin{tabular}{||l|c|c||}
		\cline{2-3}
		\multicolumn{1}{c|}{} & Relative error 1\% & Absolute Error 0.001 \\ \hline \hline
		$n_{Trap}$ & 16	 & 40 \\ \hline
		$n_{Simpson}$ & 3 & 4 \\ \hline
		$n_{Midpoint}$ & 11 & 28 \\ \hline
		$n_{Open2}$ & 9 & 23 \\ \hline
		$n_{Gauss-Legendre}$ & 8 & 4 \\ \hline
	\end{tabular}
	\caption{Number of sub-intervals for relative error of 1\% and absolute error of 0.001}
\end{table}
where we use $\max \abs{f^{\prime\prime}(\mu)}$ and $\max \abs{f^{\prime\prime\prime\prime}(\mu)}$ to get a conservative bound. We get the following results 
\begin{table}[H]
	\centering
	\begin{tabular}{||c|c|c|c|c|c|c|c|c|c||}
	\hline
	$I_{16}^{Trap}$ & $I_{213}^{Trap}$ & $I_{2}^{Simpson}$ & $I_{7}^{Simpson}$ & $I_{11}^{Midpoint}$ & $I_{151}^{Midpoint}$ & $I_{9}^{Open2}$ & $I_{123}^{Open2}$ & $I_8^{GL}$ & $I_4^{GL}$ \\ \hline \hline
		.0017603 & .000281 & .000195 & .000065 & .001866 & .000288 & .001859 & .000284 & .000002 & .000042 \\ \hline
	\end{tabular} 
	\caption{Errors of predicted $n$ for relative and absolute error}
\end{table}
It is easy to believe to see that these $n$ values satisfy our error requirements. It is easy to believe this since we used $\max f^{\prime\prime}(\mu)$ and $\max f^{\prime\prime\prime\prime}(\mu)$ to determine $n$ which yields a conservative number.
\begin{table}[H]
	\centering
	\begin{tabular}{||l|c|c|c|c|c||}
		\cline{2-6}
		\multicolumn{1}{c|}{} & Trapezoidal & Simpson & Midpoint & Open 2 & Gauss Legendre \\ \hline \hline
		$n=1/1$ & -0.0988143 & 0.796946 & 1.24483 & 1.009623 & 0.611318 \\ \hline
		$n=4/9$ & 0.660309 & 0.688786 & 0.691511 & 0.69823 & 0.688679 \\ \hline
		$n=16/81$ & 0.686961 & 0.688722 & 0.688756 & 0.689308 & 0.688721 \\ \hline
		$n=64/729$ & 0.688611 & 0.688721 & 0.688722 & 0.688758 & 0.688721 \\ \hline
	\end{tabular}
	\caption{Results of quadrature with refinement. First $n$ is for methods with $\alpha = 1/2$ (and Gauss-Legendre) and second $n$ is for methods with $\alpha = 1/3$.}
\end{table}

\newpage

\subsection{Function 3}

For $f(x) = tanh(x)$ we have that
\begin{align} 
I = \int_{-2}^1 tanh(x) \, dx = -0.89122192
\end{align}
where we use this to compare our quadrature results. If we want a relative error of 1\%, then our quadrature result $I_n$ must be in the interval $[.99I, 1.01I] \approx [-0.90013414, -0.8823097]$ and if we want a relative error of 0.001 we want our quadrature $I_n$ to be in the interval $[-0.89222192,-0.89022192]$. Estimating amount of sub-intervals via equations (7), (14), (21), (28), and (38) for relative error of 1\% and absolute error of 0.001 we get

\begin{table}[H]
	\centering
	\begin{tabular}{||l|c|c||}
		\cline{2-3}
		\multicolumn{1}{c|}{} & Relative error 1\% & Absolute Error 0.001 \\ \hline \hline
		$n_{Trap}$ & 9	 & 42 \\ \hline
		$n_{Simpson}$ & 3 & 5 \\ \hline
		$n_{Midpoint}$ & 10 & 30 \\ \hline
		$n_{Open2}$ & 9 & 24 \\ \hline
		$n_{Gauss-Legendre}$ & 3 & 4 \\ \hline
	\end{tabular}
	\caption{Number of sub-intervals for relative error of 1\% and absolute error of 0.001}
\end{table}
where we use $\max \abs{f^{\prime\prime}(\mu)}$ and $\max \abs{f^{\prime\prime\prime\prime}(\mu)}$ to get a conservative bound. We get the following results 
\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c||}
		\hline
		 $I_{16}^{Trap}$ & $I_{213}^{Trap}$ & $I_{2}^{Simpson}$ & $I_{7}^{Simpson}$ & $I_{11}^{Midpoint}$ & $I_{151}^{Midpoint}$ & $I_{9}^{Open2}$ & $I_{123}^{Open2}$ & $I_3^{GL}$ & $I_4^{GL}$ \\ \hline \hline
		 .003228 & .000149 & .000186 & .000020 & .001306 & .000145 & .001075 & .000151 & .000127 & .000030 \\ \hline
	\end{tabular} 
	\caption{Errors of predicted $n$ for relative and absolute error}
\end{table}
It is easy to believe to see that these $n$ values satisfy our error requirements. It is easy to believe this since we used $\max f^{\prime\prime}(\mu)$ and $\max f^{\prime\prime\prime\prime}(\mu)$ to determine $n$ which yields a conservative number.
\begin{table}[H]
	\centering
	\begin{tabular}{||l|c|c|c|c|c||}
		\cline{2-6}
		\multicolumn{1}{c|}{} & Trapezoidal & Simpson & Midpoint & Open 2 & Gauss Legendre \\ \hline \hline
		$n=1/1$ & -0.30365 & -1.02545 & -1.38635 & -1.14239 & -0.790909 \\ \hline
		$n=4/9$ & -0.875024 & -0.891177 & -0.892833 & -0.896595 & -0.891252 \\ \hline
		$n=16/81$ & -0.890199 & -0.891222 & -0.891242 & -0.891563 & -0.891222 \\ \hline
		$n=64/729$ & -0.891158 & -0.891222 & -0.891222 & -0.891243 & -0.891222 \\ \hline
	\end{tabular}
	\caption{Results of quadrature with refinement. First $n$ is for methods with $\alpha = 1/2$ (and Gauss-Legendre) and second $n$ is for methods with $\alpha = 1/3$.}
\end{table}	

\newpage

\subsection{Function 4}

For $f(x) = xcos(2\pi x)$ we have that
\begin{align} 
I = \int_{0}^{3.5} xcos(2\pi x) \, dx = -0.05066059
\end{align}
where we use this to compare our quadrature results. If we want a relative error of 1\%, then our quadrature result $I_n$ must be in the interval $[.99I, 1.01I] \approx [-0.0511672, -0.05015398]$ and if we want a relative error of 0.001 we want our quadrature $I_n$ to be in the interval $[-0.05166059,-0.049966059]$. Estimating amount of sub-intervals via equations (7), (14), (21), (28), and (38) for relative error of 1\% and absolute error of 0.001 we get

\begin{table}[H]
	\centering
	\begin{tabular}{||l|c|c||}
		\cline{2-3}
		\multicolumn{1}{c|}{} & Relative error 1\% & Absolute Error 0.001 \\ \hline \hline
		$n_{Trap}$ & 988 & 703 \\ \hline
		$n_{Simpson}$ & 38 & 32 \\ \hline
		$n_{Midpoint}$ & 699 & 497 \\ \hline
		$n_{Open2}$ & 570 & 406 \\ \hline
		$n_{Gauss-Legendre}$ & 28 & 29 \\ \hline
	\end{tabular}
	\caption{Number of sub-intervals for relative error of 1\% and absolute error of 0.001}
\end{table}
where we use $\max \abs{f^{\prime\prime}(\mu)}$ and $\max \abs{f^{\prime\prime\prime\prime}(\mu)}$ to get a conservative bound. We get the following results
\begin{table}[H]
	\centering
	\begin{tabular}{||c|c|c|c|c|c|c|c|c|c||}
		\hline
		$I_{16}^{Trap}$ & $I_{213}^{Trap}$ & $I_{2}^{Simpson}$ & $I_{7}^{Simpson}$ & $I_{11}^{Midpoint}$ & $I_{151}^{Midpoint}$ & $I_{9}^{Open2}$ & $I_{123}^{Open2}$ & $I_{28}^{GL}$ & $I_{29}^{GL}$ \\ \hline \hline
		.000002 & .000004 & .000006 & .000012 & .000002 & .000005 & .000002 & .000005 & .000014 & .000012 \\ \hline
	\end{tabular} 
	\caption{Errors of predicted $n$ for relative and absolute error}
\end{table}
It is easy to believe to see that these $n$ values satisfy our error requirements. It is easy to believe this since we used $\max f^{\prime\prime}(\mu)$ and $\max f^{\prime\prime\prime\prime}(\mu)$ to determine $n$ which yields a conservative number.
\begin{table}[H]
	\centering
	\begin{tabular}{||l|c|c|c|c|c||}
		\cline{2-6}
		\multicolumn{1}{c|}{} & Trapezoidal & Simpson & Midpoint & Open 2 & Gauss Legendre \\ \hline \hline
		$n=1/1$ & -6.125 & -2.04167 & 2.62534E-12 & -1.02083 & 0.230093 \\ \hline
		$n=4/9$ & -2.61401 & 0.738683 & -0.0292888 & 1.20564 & -0.616779 \\ \hline
		$n=16/81$ & -0.0594496 & -0.0504533 & -0.050504 & -0.0476094 & -0.0507996 \\ \hline
		$n=64/729$ & -0.051162& -0.0506599 & -0.0506587 & -0.050493 & -0.0506611 \\ \hline
		$n=256/6561$ & -0.0506918 & -0.0506606 & -0.0506606 & -0.0506502 & -0.0506606 \\ \hline
		$n=1024/59049$ & -0.0506625 & -0.0506606 & -0.0506606 & -0.0506606 & -0.0506606 \\ \hline
	\end{tabular}
	\caption{Results of quadrature with refinement. First $n$ is for methods with $\alpha = 1/2$ (and Gauss-Legendre) and second $n$ is for methods with $\alpha = 1/3$.}
\end{table}	 

\newpage

\subsection{Function 5}

For $f(x) = x + \frac{1}{x}$ we have that
\begin{align} 
I = \int_{0.1}^{2.5} x + \frac{1}{x}\, dx = 6.3388758
\end{align}
where we use this to compare our quadrature results. If we want a relative error of 1\%, then our quadrature result $I_n$ must be in the interval $[.99I, 1.01I] \approx [6.27548704, 6.40226456]$ and if we want a relative error of 0.001 we want our quadrature $I_n$ to be in the interval $[6.3378758,6.3398758]$. Estimating amount of sub-intervals via equations (7), (14), (21), (28), and (38) for relative error of 1\% and absolute error of 0.001 we get

\begin{table}[H]
	\centering
	\begin{tabular}{||l|c|c||}
		\cline{2-3}
		\multicolumn{1}{c|}{} & Relative error 1\% & Absolute Error 0.001 \\ \hline \hline
		$n_{Trap}$ & 191 & 1518 \\ \hline
		$n_{Simpson}$ & 32 & 91 \\ \hline
		$n_{Midpoint}$ & 135 & 1074 \\ \hline
		$n_{Open2}$ & 111 & 877 \\ \hline
		$n_{Gauss-Legendre}$ & 39 & 82 \\ \hline
	\end{tabular}
	\caption{Number of sub-intervals for relative error of 1\% and absolute error of 0.001}
\end{table}
where we use $\max \abs{f^{\prime\prime}(\mu)}$ and $\max \abs{f^{\prime\prime\prime\prime}(\mu)}$ to get a conservative bound. We get the following results
\begin{table}[H]
	\centering
	\begin{tabular}{||c|c|c|c|c|c|c|c|c|c||}
	\hline
	 $I_{16}^{Trap}$ & $I_{213}^{Trap}$ & $I_{2}^{Simpson}$ & $I_{7}^{Simpson}$ & $I_{11}^{Midpoint}$ & $I_{151}^{Midpoint}$ & $I_{9}^{Open2}$ & $I_{123}^{Open2}$ & $I_{39}^{GL}$ & $I_{82}^{GL}$ \\ \hline \hline
	.001314 & .000024 & .000514 & .000014 & .001306 & .000016 & .001286 & .000016 & .000166 & .000006 \\ \hline
	\end{tabular} 
	\caption{Errors of predicted $n$ for relative and absolute error}
\end{table}
It is easy to believe to see that these $n$ values satisfy our error requirements. It is easy to believe this since we used $\max f^{\prime\prime}(\mu)$ and $\max f^{\prime\prime\prime\prime}(\mu)$ to determine $n$ which yields a conservative number.
\begin{table}[H]
	\centering
	\begin{tabular}{||l|c|c|c|c|c||}
		\cline{2-6}
		\multicolumn{1}{c|}{} & Trapezoidal & Simpson & Midpoint & Open 2 & Gauss Legendre \\ \hline \hline
		$n=1/1$ & 15.6 & 8.51077 & 4.96615 & 5.15922 & 5.69851 \\ \hline
		$n=4/9$ & 7.87447 & 6.53664 & 6.16254 & 5.97582 & 6.23859 \\ \hline
		$n=32/243$ & 6.38352 & 6.33939 & 6.33847 & 6.32431 & 6.33854 \\ \hline
		$n=256/6561$ & 6.33961 & 6.33888 & 6.33888 & 6.33863 & 6.33888 \\ \hline
		$n=1024/59049$ & 6.33892 & 6.33888 & 6.33888 & 6.33886 & 6.33888 \\ \hline
		$n=2048/177147$ & 6.33889 & 6.33888 & 6.33888 & 6.33887 & 6.33888 \\ \hline
	\end{tabular}
	\caption{Results of quadrature with refinement. First $n$ is for methods with $\alpha = 1/2$ (and Gauss-Legendre) and second $n$ is for methods with $\alpha = 1/3$.}
\end{table}	 

\newpage

\section{Conclusion}

Quadrature methods provide efficient and accurate results for integration over finite domains. Proper error analysis can tell the user much about what to expect
\begin{enumerate}
	\item How many sub-intervals are need for a certain error tolerance,
	\item When to expect exact answers from quadrature,
	\item What methods are better suited for the user.
\end{enumerate}
In this program we proved that the five quadrature methods have a bounded error which can be derived, certain methods provided more accurate results than other, number of sub-intervals tell us how accurate we will be, and degree of exactness allows for exact integration. In addition to this, the results from the five functions gave insight into how each method works and how efficient reuse of function evaluation can be taken advantage of. This is important because sometimes it takes a great deal of processing power, and time, to compute function evaluations. 

\end{document}
